{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaf27566",
   "metadata": {},
   "source": [
    "## FINAL PROJECT: ISM 6562 - BIG DATA FOR BUSINESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d645adf",
   "metadata": {},
   "source": [
    "### **Setup Steps**\n",
    "\n",
    "> ⚠️ **Note:** You must have a Java Development Kit (JDK) installed and the `JAVA_HOME` environment variable correctly configured.\n",
    "\n",
    "- **For Windows**: The stable JDK version as of May 3, 2025, is **Java 11**.  \n",
    "- **For macOS**: The stable version is **Java 17**, available from [Adoptium](https://adoptium.net/).\n",
    "\n",
    "The Python code below checks for required packages and installs any that are missing. This ensures all necessary libraries are available for import in your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae450ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "pandas is already installed.\n",
      "matplotlib is already installed.\n",
      "seaborn is already installed.\n",
      "ucimlrepo is already installed.\n",
      "pyspark is already installed.\n",
      "sklearn is already installed.\n",
      "findspark is already installed.\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of packages to ensure are installed\n",
    "packages = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"ucimlrepo\",\n",
    "    \"pyspark\",\n",
    "    \"sklearn\",       # scikit-learn's import name is 'sklearn'\n",
    "    \"findspark\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    else:\n",
    "        print(f\"{package} is already installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db89d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"330\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#46327e;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#365c8d;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#277f8e;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#1fa187;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#4ac16d;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#a0da39;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "[(0.275191, 0.194905, 0.496005),\n",
       " (0.212395, 0.359683, 0.55171),\n",
       " (0.153364, 0.497, 0.557724),\n",
       " (0.122312, 0.633153, 0.530398),\n",
       " (0.288921, 0.758394, 0.428426),\n",
       " (0.626579, 0.854645, 0.223353)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import findspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "sns.color_palette(palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993c042",
   "metadata": {},
   "source": [
    "### Import Data from UCI ML dataset library\n",
    "Importing as a pandas dataframe as this is how the data is accessible from the UCI ML Repo. For cases where there was a larger dataset, a new pipeline into pyspark would further need to be developed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ea729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baker\\anaconda3\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "findspark.init() \n",
    "# If you have a SPARK_HOME environment variable set, it might find it automatically\n",
    "spark_path = findspark.find()\n",
    "print(spark_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d0f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "communities_and_crime = fetch_ucirepo(id=183) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = communities_and_crime.data.features \n",
    "y = communities_and_crime.data.targets \n",
    "\n",
    "df = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1ddf0",
   "metadata": {},
   "source": [
    "##### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd37aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cast_expr = \",\\n\".join(\n",
    "    [f\"CAST({col} AS DOUBLE) AS {col}\" for col in selected_cols]\n",
    ")\n",
    "\n",
    "# Convert the merged pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149878a",
   "metadata": {},
   "source": [
    "spark_df.printSchema()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
